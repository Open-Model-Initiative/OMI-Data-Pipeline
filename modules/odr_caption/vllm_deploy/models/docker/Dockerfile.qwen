# Use the official vLLM image as the base
FROM vllm/vllm-openai:latest

# Set the working directory
WORKDIR /vllm-workspace

# Install additional dependencies for Qwen models
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install accelerate \
    bitsandbytes>=0.44.0 \
    git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830 \
    hf_transfer \
    'modelscope!=1.15.0' \
    timm==0.9.10
# Install flash-attn separately
# RUN pip install --no-cache-dir flash-attn --no-build-isolation

# Set environment variables
ENV VLLM_USAGE_SOURCE custom-qwen-docker-image

# The entrypoint remains the same as the original
ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]
